---
title: "ModelBuilding"
author: "Victor"
date: "April 2, 2017"
output: html_document
---
```{r load packages}
#source("https://bioconductor.org/biocLite.R")
#biocLite('e1071')                                    # required for glmnet in caret
#biocLite('pROC')
library(pROC)
library(ggplot2)
library(limma)
library(caret)
library(dplyr)
library(parallel)
library(doParallel)
```

```{r parallel processing}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```

Read in pre-processed data:
*Make sure the pre-processed data (data.txt, which is in data.zip) is present in the ../processed_data/ directory.

```{r load data}
#setwd('../')                                           # note: all of these relative file path calls work only for knitting

# load data (pre-processed training set)
train.data <- read.table('./data/Processed Data/data.txt')
str(train.data)                                   ## row names are CpG sites, column names are sample names
# transpose our data to have rows as observations, which is more convenient later on for building models
train.data <- as.data.frame(t(train.data))

# load metadata
design <- read.csv("./data/Processed Data/des.txt", sep="\t", header=TRUE)
str(design)

row.names(train.data) == design$Samplename               # check that the samples are in same order
```

Read in test data:
```{r read test}
# read pre-processed test data
test.data <- read.table("./Data/Processed Data/Test data/Matrix.processed.betas.placenta.txt", row.names = 1, header = T)
test.data <- as.data.frame(t(test.data))   #transpose data
```

We should remove any sites with NAs in them or else predictions cannot be generated for these samples if the CpG site is chosen as a predictor
```{r remove NAs}
#remove sites with NAs
sum(is.na(test.data)) # 52000 total entries that are NA
test.rmna <- test.data[, colSums(is.na(test.data)) == 0]  # remove columns with NAs present
```

```{r filter training data to contain the same CpGs as the test}
# this isn't necessary if the test data didn't have CpGs removed (as a result of QC/preprocessing)
train.data <- train.data[,colnames(train.data) %in% colnames(test.rmna)]
```

## Prefiltering cpgs 
The goal of this prefiltering section is to reduce computational time without compromising detecting interesting features.

```{r prefiltering based on SD}
train.sd <- apply(as.matrix(train.data), MARGIN = 2,FUN = sd) #caculate SD for each feature
sd(train.sd)
hist(train.sd)                    # histogram
abline(v = mean(train.sd)) 

# filter CpG sites with low s.d: only keep those with s.d higher than the average s.d across all CpG sites
train.gsd <- subset(train.sd, train.sd > 0.10)
hist(train.gsd)

train.data.gsd <- train.data[,colnames(train.data) %in% names(train.gsd)]
```

#### We reduced the # of features to 'r ncol(train.data.gsd)' to reduce computation time. train.data.gsd is the working dataset
# Step 2: Supervised classification:


## 2.1 logistic regression with elastic net regularization
```{r rename}
#renamed just so that I can copy Amrit's code
x.train <- train.data.gsd
y.train <- design$Ethnicity 
```

```{r subset data for faster run time, eval = FALSE, include = FALSE}
# This subsetting is for testing code out without computational delay
# since the data is very large (~450k rows), I will subset the data first to be able to play around with the code quickly.
x.train <- train.data.gsd[,1:1000] #takes the first 1000 columns (cpgs)
```

```{r Specify resampling method}
k = 5
M = 3

fitControl <- trainControl(method = "repeatedcv", 
													 number = k,                 # Number of folds
													 repeats = M,
													 ## Estimate class probabilities
													 classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary,
													 allowParallel = TRUE
													 )  

netGrid <- expand.grid(alpha = c(0.75),
                           lambda = c(0.077, 0.25))
```


```{r tune glmnet parameters}
set.seed(2017)                                         # training models requires the use of random #s. Setting (set.seed()) the randomness ensures reproducibility

#netGrid <- expand.grid(.alpha = seq(.05, 1, length = 15),
 #                                                   .lambda = c((1:5)/10)) # grid of tuning parameters to try out

system.time(netFit <- train(x = x.train,   # samples need to be in rows, features need to be columns
								y = y.train,                  
								method = "glmnet",                     # glmnet model
								trControl = fitControl,                # use fitControl to specify cross validation
								tuneGrid = netGrid,
								preProcess = c( "center", "scale"),    # Center and Scale the data
								metric = 'Accuracy')                        # ROC because distribution is slightly skewed
)

netFit
#saveRDS(netFit, './Data/Processed Data/netFitfinal.rds')
```

```{r extract features}
predictorsNet <- predictors(netFit)
length(predictorsNet)
write.table(predictorsNet, './Data/Processed Data/predictorsGlmnet.txt')
```
Looks like our glmnet-built model has chosen 'r length(predictors)' CpGs that can be used to predict ethnicity.

## 2.2 SVM linear
This section is for building the model using SVM. However, because computational time is long, this section is can be excluded if chosen (specify eval = false)

```{r svm linear, eval = FALSE, include = FALSE}
svmControl <- trainControl(method="repeatedcv",   
                           number = 5,
                           repeats=3,		    
                           summaryFunction=twoClassSummary,	# Use AUC to pick the best model
                           classProbs=TRUE,
                           allowParallel = TRUE)

system.time(svmFit <- train(x=x.train,
                            y= y.train,
                            method = "svmLinear",
                            preProc = c("center","scale"),
                            metric="ROC",
                            trControl= svmControl)	)
svmFit
```
# Step 3: Predict Ethnicity for Test Set

## 3.1 glmnet
```{r predict ethnicity for test set}
#subset x.test down to the sites used for training (after prefilter)
x.test <- test.data[,colnames(test.data) %in% names(x.train)]

y.predictNet <- predict(netFit,  x.test)
y.predictNet
saveRDS(y.predictNet, './data/Processed Data/y_predictNet.rds')
```
## 3.2 SVM 

```{r SVM predict on test, eval = FALSE}
y.predictSVM <- predict(svmFit,  x.test)
#throws a warning
y.predictSVM
```


# 4.0 Analysis of Predictors

## 4.1 Clustering

```{r Clustering test data}
# Hierarchical clustering of predicted data, distance measured by Euclidean distance, average linkage
hclust <- hclust(dist(x.test, method = 'euclidean'))

library(sparcl) # for coloring dendrogram
# colors the leaves of a dendrogram
y = cutree(hclust, 2)
ColorDendrogram(hclust, y=y, labels = names(y), branchlength = 2)
```

```{r clustering train based on predictors}
x.train.predictors <- x.train[,colnames(x.train) %in% predictorsNet]
x.train.predictors
x.train[,]
```
##
```{r plot top 35}
glmImp <- varImp(netFit, scale = F) # gives the t-statistic for all CpGs in the dataset
plot(glmImp, top = 51)
```

### This section is to tune parameters across 100 different combinations of alpha and lambda

```{r tune glmnet parameters 10 by 10, eval = FALSE, include = FALSE}
netGrid100 <-  expand.grid(alpha = c(0.20, 0.40, 0.60, 0.80, 1.00),
                           lambda = c(0.05, 0.10, 0.15, 0.20, 0.25))

set.seed(2017)                                         # training models requires the use of random #s. Setting (set.seed()) the randomness ensures reproducibility

#netGrid <- expand.grid(.alpha = seq(.05, 1, length = 15),
 #                                                   .lambda = c((1:5)/10)) # grid of tuning parameters to try out

system.time(netFit100 <- train(x = x.train,   # samples need to be in rows, features need to be columns
								y = y.train,                  
								method = "glmnet",                     # glmnet model
								trControl = fitControl,                # use fitControl to specify cross validation
								tuneGrid = netGrid100,
								preProcess = c( "center", "scale"),    # Center and Scale the data
								metric = 'ROC')                        # ROC because distribution is slightly skewed
)

```

```{r examine CV, eval = FALSE}
trellis.par.set(caretTheme())
ggplot(netFit100)
#heatmap of results
plot(netFit100, metric = "ROC", plotType = "level",
     scales = list(x = list(rot = 90)))
length(predictors(netFit100))
glmImp100 <- varImp(netFit100, scale = F) # gives the t-statistic for all CpGs in the dataset
plot(glmImp100, top = 50)
```