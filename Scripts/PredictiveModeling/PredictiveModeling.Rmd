---
title: "Predictive Modeling Analysis"
author: "Ming Wan, Victor Yuan"
output: 
  github_document:
    toc: TRUE
---

# Step 0: Load Packages and Data
Load required packages:


```{r load packages}
#source("https://bioconductor.org/biocLite.R")
#biocLite('e1071')                                    # required for glmnet in caret
#biocLite('pROC')
library(pROC)
library(ggplot2)
library(limma)
library(caret)
library(dplyr)
#library(glmnet)
```

Read in pre-processed data:
*Make sure the pre-processed data (data.txt, which is in data.zip) is present in the ../processed_data/ directory.

```{r load data}
setwd('../')                                           # note: all of these relative file path calls work only for knitting

# load data (pre-processed training set)
train.data <- read.table('../Data/Processed Data/data.txt')
str(train.data)
## row names are CpG sites, column names are sample names

# load metadata
design <- read.csv("../Data/Processed Data/des.txt", sep="\t", header=TRUE)
str(design)

colnames(train.data) == design$Samplename               # check that the samples are in same order
```

# Step 1: Unsupervised clustering:
---------------this section is exploratory analysis, we should move this to Nivi's exploratory file----------
As Rob suggested, PCA should be the precursor to supervised classification, more like an exploration.

## PCA on training data:

```{r pca}
pc.train <- prcomp(t(scale(t(train.data), center = T, scale = T)), center = F, scale = F)

# look at the eigenvalues
plot(pc.train) # can we add some labels here (PC1, PC2, PC3,...)
```
Say something about the plot 

```{r not sure what this chunk of code tells us, eval = FALSE, include = FALSE}
pc.train$sdev
diag((pc.train$sdev)^2)
sum(diag((pc.train$sdev)^2))
diag((pc.train$sdev)^2)[1,1]/sum(diag((pc.train$sdev)^2))
```

```{r Plot PC1 PC2 PC3}
# first 2 PCS
PC123 <- data.frame(pc.train$rotation[,c("PC1","PC2", "PC3")])              # Take out first 3 PCs
PC123 <- PC123 %>% tibble::rownames_to_column('Samplename') %>%             # Put sample names into column to 
                    left_join(design, 'Samplename')                         # Join the metadata info 
head(PC123)            

ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = Ethnicity)) +
  ggtitle('PC1 and PC2: Ethnicity')

ggplot(PC123, aes(x = PC1, y = PC3)) + 
  geom_point(aes(color = Ethnicity)) +
  ggtitle('PC1 and PC3: Ethnicity')

ggplot(PC123, aes(x = PC2, y = PC3)) + 
  geom_point(aes(color = Ethnicity)) +
  ggtitle('PC2 and PC3: Ethnicity')
```
We can see from plotting the first three principal components that our groups (Asian, Caucasian) do not seem to separate. This indicates that the main drivers of the variance in the data is something else.

```{r Plot other metadata}
ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = sex)) +
  ggtitle('Sex')

ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = ga)) +
  ggtitle('Gestational Age')

ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = Sample_Group)) +
  ggtitle('Sample Group')
```
It's not clear that our other variables are driving the variance in the data (sex, gestational age, and sample group).

```{r Scatter plot matrix first 5 PCs}
# scatter plot matrix for the first 5 PCs
splom(pc.train$rotation[,1:5], panel = panel.smoothScatter, raster = TRUE)
```
What does this plot show?

## PCA projection of loadings to test data:

```{r}
# read pre-processed test data

# project PC loadings to test data

```

--------End of exploratory Analysis-----------

# Step 2: Supervised classification:


## logistic regression with elastic net regularization
```{r}
#renamed just so that I can copy Amit's code

x.train <- train.data 
y.train <- design$Ethnicity 
```

```{r subset data for faster run time, eval = FALSE, include = FALSE}
# since the data is very large (~450k rows), I will subset the data first to be able to play around with the code quickly.

x.train <- train.data[1:1000,] #takes the first 1000 rows=
```

```{r Specify resampling method}
k = 5
M = 3

fitControl <- trainControl(method = "repeatedcv", 
													 number = k,                 # Number of folds
													 repeats = M,
													 ## Estimate class probabilities
													  classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary,
													 savePredictions = TRUE      # Saves ROC results
													 )  
```


```{r tune glmnet parameters}
set.seed(2017)                                         # training models requires the use of random #s. Setting (set.seed()) the randomness                                                             ensures reproducibility

system.time(netFit <- train(x = t(x.train),   # samples need to be in rows, features need to be columns
								y = y.train,                  
								method = "glmnet",                     # glmnet model
								trControl = fitControl,                # use fitControl to specify cross validation
								preProcess = c( "center", "scale"),    # Center and Scale the data
								metric = 'ROC')                        # ROC because distribution is slightly skewed
)

netFit
```
Cross validation with a fold of k = 5 (making each fold 9 samples large), was used to determine the optimal tuning parameters.

Horvath et al. (2013) uses an 'elastic net generalized linear model' to build an across-tissue DNAm predictor on age. Since our data is the same type, we'll try glmnet.

Horvath, S. (2013). DNA methylation age of human tissues and cell types. Genome Biology, 14(10), R115. http://doi.org/10.1186/gb-2013-14-10-r115
 
```{r examine results}
trellis.par.set(caretTheme())
ggplot(netFit)

#heatmap of results
plot(netFit, metric = "ROC", plotType = "level",
     scales = list(x = list(rot = 90)))
```

```{r extract features}
predictors <- predictors(netFit)
predictors
length(predictors) 
```
Looks like our model has chosen 'r length(predictors)' CpGs that can be used to predict ethnicity.

```{r plot top 35}
glmImp <- varImp(netFit, scale = F) # gives the t-statistic for all CpGs in the dataset
plot(glmImp, top = 35)
```

```{r Estimate test error by nested cv}
# this is adapted from Amit's code from lec 19
#use repeated CV to estimate test performance
set.seed(2018)

# list of lists containing fold ids
folds <- lapply(1:M, function(i) createFolds(y.train, k = k))

netTesterror <- lapply(folds, function(i){
  lapply(i, function(j){
    # tune parameters with CV
    set.seed(2019)
    fitControl <- trainControl(method = "repeatedcv", 
													 number = k,                 
													 repeats = M,
													 classProbs = TRUE,
                           summaryFunction = twoClassSummary,
													 savePredictions = TRUE)
    
    
    # build elastic net classifier
    netFit <- train(x =  t(x.train)[-j,],   
								y = y.train[-j],                  
								method = "glmnet",                     
								trControl = fitControl,
								preProcess = c( 'center', 'scale'),
								metric = 'ROC')   
    
    # Estimate probabilities of test predictions
    probTest <- predict(netFit, t(x.train)[j,], type = 'prob')
    ethProb <- probTest[,'Asian']
    ethProb

  })
})
netTesterror
```

```{r Performance}
# Computer classification performance measures
# enet
Performance <- mapply(function(x, y){
  auc <- pROC::roc(y.train[unlist(x)], unlist(y),
                   direction ='<',
                   levels = c('Caucasian', 'Asian'),
                   percent = TRUE)
  list(tpr = auc$sensitivities,
       fpr = 100 - auc$specificities,
       auc = round(auc$auc, 2))
}, x = folds, y = netTesterror)
Performance
```

```{r ROC curve}
# plot ROC curve

plot(Performance['tpr',][[1]] ~ Performance['fpr',][[1]],
     type = 'l', col = 1, xlab = '100 - sensitivity',
     ylab = 'Sensitivity', main = 'Enet')
for(i in length(folds)){
  points(Performance['tpr',][[i]] ~ Performance['fpr',][[i]],
         type = 'l', col = 2)
}
text(x = 60, y = 40, labels =
       paste0('mean AUC = ', round(mean(unlist(Performance['auc',])), 1),
              '+/-', round(sd(unlist(Performance['auc',])), 1), '%'))
```