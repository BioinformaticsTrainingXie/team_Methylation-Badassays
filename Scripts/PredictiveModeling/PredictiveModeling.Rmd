---
title: "Predictive Modeling Analysis"
author: "Ming Wan, Victor Yuan"
output: 
  github_document:
    toc: TRUE
---

# Step 0: Load Packages and Data
Load required packages:


```{r load packages}
#source("https://bioconductor.org/biocLite.R")
#biocLite('e1071')                                    # required for glmnet in caret
#biocLite('pROC')
library(pROC)
library(ggplot2)
library(limma)
library(caret)
library(dplyr)
#library(glmnet)
library(devtools)
install_github("ggbiplot", "vqv")
library(ggbiplot)
library(parallel)
library(foreach)
library(doParallel)
```

```{r create multiple processes}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```

Read in pre-processed data:
*Make sure the pre-processed data (data.txt, which is in data.zip) is present in the ../processed_data/ directory.

```{r load data}
setwd('../')                                           # note: all of these relative file path calls work only for knitting

dimension <- c(464923, 45) # to speed up data read-in
# load data (pre-processed training set)
train.data <- read.table('../data/Processed Data/data.txt', colClasses = rep("numeric",dimension[2]),nrows = dimension[1])
str(train.data)
## row names are CpG sites, column names are sample names
# transpose our data to have rows as observations, which is more convenient later on for building models
train.data <- as.data.frame(t(train.data))


# load metadata
design <- read.csv("../data/Processed Data/des.txt", sep="\t", header=TRUE)
str(design)

colnames(train.data) == design$Samplename               # check that the samples are in same order
```

```{r}
train.sd <- apply(as.matrix(train.data), MARGIN = 2,FUN = sd)
sd(train.sd)

hist(train.sd)
abline(v = mean(train.sd))

# filter CpG sites with low s.d: only keep those with s.d higher than the average s.d across all CpG sites
train.gsd <- subset(train.sd, train.sd > mean(train.sd))
str(train.gsd)
hist(train.gsd)

```

Read in test data:
```{r}
# read pre-processed test data
test.data <- read.table("../data/Processed Data/Test data/GSE69502_Matrix.processed.detP.txt", row.names = 1, header = T)

```


# Step 1: Unsupervised clustering:
---------------this section is exploratory analysis, we should move this to Nivi's exploratory file----------
As Rob suggested, PCA should be the precursor to supervised classification, more like an exploration.

## PCA on training data:

```{r pca}
pc.train <- prcomp(scale(train.data,center=T,scale = T))

# look at the eigenvalues
plot(pc.train, type = "l")
```
The `plot()` function returns a plot of the variances (y-axis) associated with the PCs (x-axis), which is useful to decide how many PCs to retain for further analysis.

```{r pca.summary}
summary(pc.train)
```
The `summary()` function describes the importance of the PCs. The first row describe again the standard deviation associated with each PC. The second row shows the proportion of the variance in the data explained by each component while the third row describe the cumulative proportion of explained variance.

```{r plot PCs}
g <- ggbiplot(pc.train, obs.scale = 1, var.scale = 1, 
              groups = design$Ethnicity, ellipse = TRUE, 
              circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', 
               legend.position = 'top')
print(g)



PC123 <- data.frame(pc.train$x[,1:5])              # Take out first 3 PCs
PC123 <- PC123 %>% tibble::rownames_to_column('Samplename') %>%             # Put sample names into column to 
                    left_join(design, 'Samplename')                         # Join the metadata info 
head(PC123)            

ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = Ethnicity)) +
  ggtitle('PC1 and PC2: Ethnicity')

ggplot(PC123, aes(x = PC1, y = PC3)) + 
  geom_point(aes(color = Ethnicity)) +
  ggtitle('PC1 and PC3: Ethnicity')

ggplot(PC123, aes(x = PC2, y = PC3)) + 
  geom_point(aes(color = Ethnicity)) +
  ggtitle('PC2 and PC3: Ethnicity')
```
We can see from plotting the first three principal components that our groups (Asian, Caucasian) do not seem to separate. This indicates that the main drivers of the variance in the data is something else.

```{r Plot other metadata}
ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = sex)) +
  ggtitle('Sex')

ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = ga)) +
  ggtitle('Gestational Age')

ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = Sample_Group)) +
  ggtitle('Sample Group')
```
It's not clear that our other variables are driving the variance in the data (sex, gestational age, and sample group).

```{r Scatter plot matrix first 5 PCs}
# scatter plot matrix for the first 5 PCs
splom(PC123[,c(2:6,10)], panel = panel.smoothScatter, raster = TRUE)
```
Plotting scatter plots of the top 5 PCs against ethnicity, none of the PCs can clearly separate samples by ethnicity, disappointing.

## PCA projection of loadings to test data:

We can use the predict function if we observe new data and want to predict their PCs values.

```{r}
# predict PCs for test set by projecting PC loadings to test data
predict(pc.train, new.data = test.data)
```

--------End of exploratory Analysis-----------

# Step 2: Supervised classification:


## logistic regression with elastic net regularization
```{r}
#renamed just so that I can copy Amrit's code

x.train <- train.data 
y.train <- design$Ethnicity 
```

```{r subset data for faster run time, eval = FALSE, include = FALSE}
# since the data is very large (~450k rows), I will subset the data first to be able to play around with the code quickly.

x.train <- train.data[1:1000,] #takes the first 1000 rows=
```

```{r Specify resampling method}
k = 5
M = 3

fitControl <- trainControl(method = "repeatedcv", 
													 number = k,                 # Number of folds
													 repeats = M,
													 ## Estimate class probabilities
													  classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary,
													 savePredictions = TRUE,      # Saves ROC results
													 allowParallel = TRUE
													 )  
```


```{r tune glmnet parameters}
set.seed(2017)                                         # training models requires the use of random #s. Setting (set.seed()) the randomness                                                             ensures reproducibility

system.time(netFit <- train(x = x.train,   # samples need to be in rows, features need to be columns
								y = y.train,                  
								method = "glmnet",                     # glmnet model
								trControl = fitControl,                # use fitControl to specify cross validation
								preProcess = c( "center", "scale"),    # Center and Scale the data
								metric = 'ROC')                        # ROC because distribution is slightly skewed
)

netFit
```
Cross validation with a fold of k = 5 (making each fold 9 samples large), was used to determine the optimal tuning parameters.

Horvath et al. (2013) uses an 'elastic net generalized linear model' to build an across-tissue DNAm predictor on age. Since our data is the same type, we'll try glmnet.

Horvath, S. (2013). DNA methylation age of human tissues and cell types. Genome Biology, 14(10), R115. http://doi.org/10.1186/gb-2013-14-10-r115
 
```{r examine results}
trellis.par.set(caretTheme())
ggplot(netFit)

#heatmap of results
plot(netFit, metric = "ROC", plotType = "level",
     scales = list(x = list(rot = 90)))
```

```{r extract features}
predictors <- predictors(netFit)
predictors
length(predictors) 
```
Looks like our model has chosen 'r length(predictors)' CpGs that can be used to predict ethnicity.

```{r plot top 35}
glmImp <- varImp(netFit, scale = F) # gives the t-statistic for all CpGs in the dataset
plot(glmImp, top = 35)
```

```{r Estimate test error by nested cv}
# this is adapted from Amrit's code from lec 19
#use repeated CV to estimate test performance
set.seed(2018)

# list of lists containing fold ids
folds <- lapply(1:M, function(i) createFolds(y.train, k = k))

system.time(netTesterror <- lapply(folds, function(i){
  lapply(i, function(j){
    # tune parameters with CV
    set.seed(2019)
    fitControl <- trainControl(method = "repeatedcv", 
													 number = k,                 
													 repeats = M,
													 classProbs = TRUE,
                           summaryFunction = twoClassSummary,
													 savePredictions = TRUE, allowParallel = T)
    
    
    # build elastic net classifier
    netFit <- train(x =  x.train[-j,],   
								y = y.train[-j],                  
								method = "glmnet",                     
								trControl = fitControl,
								preProcess = c( 'center', 'scale'),
								metric = 'ROC')   
    
    # Estimate probabilities of test predictions
    probTest <- predict(netFit, x.train[j,], type = 'prob')
    ethProb <- probTest[,'Asian']
    ethProb

  })
})
)
netTesterror
```

```{r Performance}
# Computer classification performance measures
# enet
Performance <- mapply(function(x, y){
  auc <- pROC::roc(y.train[unlist(x)], unlist(y),
                   direction ='<',
                   levels = c('Caucasian', 'Asian'),
                   percent = TRUE)
  list(tpr = auc$sensitivities,
       fpr = 100 - auc$specificities,
       auc = round(auc$auc, 2))
}, x = folds, y = netTesterror)
Performance
```

```{r ROC curve}
# plot ROC curve

plot(Performance['tpr',][[1]] ~ Performance['fpr',][[1]],
     type = 'l', col = 1, xlab = '100 - sensitivity',
     ylab = 'Sensitivity', main = 'Enet')
for(i in length(folds)){
  points(Performance['tpr',][[i]] ~ Performance['fpr',][[i]],
         type = 'l', col = 2)
}
text(x = 60, y = 40, labels =
       paste0('mean AUC = ', round(mean(unlist(Performance['auc',])), 1),
              '+/-', round(sd(unlist(Performance['auc',])), 1), '%'))
```



# Step 3: Predict Ethnicity for Test Set

Use eirarchical clustering to visualize the cluster 
```{r predict ethnicity for test set}
y.predict <- predict(netFit, newdata = x.test)

# Hierarchical clustering of predicted data
distance <- dist(y.predict, method="euclidean") 
cluster <- hclust(distance, method="average")
plot(cluster, hang=-1, label=y.predict)

```

# Step 4: Differentially Methylated Sites for Test Set

```{r}

```

