---
title: "Predictive Modeling Analysis"
author: "Ming Wan, Victor Yuan"
output: 
  github_document:
    toc: TRUE
---

Contents in this file are copied to and tidied in BuildModel_AnalyzePredictors.Rmd and Exploratory.Rmd files.
# Step 0: Load Packages and Data
Load required packages:

```{r load packages}
#source("https://bioconductor.org/biocLite.R")
#biocLite('e1071')                                    # required for glmnet in caret
#biocLite('pROC')
library(pROC)
library(ggplot2)
library(limma)
library(caret)
library(dplyr)
#library(glmnet)
library(devtools)
library(kernlab)
install_github("ggbiplot", "vqv")
library(ggbiplot)
library(parallel)
library(foreach)
library(doParallel)
library(readxl)
```

```{r create multiple processes}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```

Read in pre-processed training data:
*Make sure the pre-processed data (data.txt, which is in data.zip) is present in the ../processed_data/ directory.

```{r load training data}
#setwd('../')                                           # note: all of these relative file path calls work only for knitting

# load data (pre-processed training set)
train.data <- read.table('../../data/Processed Data/data.txt')
str(train.data)
## row names are CpG sites, column names are sample names
# transpose our data to have rows as observations, which is more convenient later on for building models
train.data <- as.data.frame(t(train.data))


# load metadata
train.design <- read.csv("../../data/Processed Data/des.txt", sep="\t", header=TRUE)
str(train.design)

row.names(train.data) == train.design$Samplename               # check that the samples are in same order
```

Read in test data:
```{r load test data}
# read pre-processed test data
test.data <- read.table("../../Data/Processed Data/Test data/Matrix.processed.betas.placenta.txt", row.names = 1, header = T)
test.data <- as.data.frame(t(test.data))
sum(is.na(test.data)) # 52000 total entries that are NA
test.rmna <- test.data[, colSums(is.na(test.data)) == 0]  # remove columns with NAs present

test.design <-  read_excel("../../data/Processed Data/Test data/metadata.GA_illumina_methylation.xls", 
    sheet = "Metadata", skip = 28)
str(test.design)
## had to change one rowname in .xls file
#rownames(test.design)[which(test.design$`Sample name` == "mt4-5_v")] <- "mt4.5_v"
# subset columns we need and rename them
test.design <- test.design[test.design$`Sample name` %in% rownames(test.data),]
test.design <- test.design[,c(1,7,8,10)]
colnames(test.design)[1] <- "Samplename"
colnames(test.design)[3] <- "sex"
colnames(test.design)[4] <- "ga"

str(test.design)

```

```{r filter training data to contain the same CpGs as the test}
train.data <- train.data[,colnames(train.data) %in% colnames(test.rmna)]
dim(train.data)
```

We only keep CpG sites in training set that: 

* are also present in the test set

* does not have missing values in the test set

Which leaves `r dim(train.data)[2]` present in training set.

## Prefiltering cpgs 

It takes way too long to include all 400k sites when fitting elastic net logistic regression and SVM, so we will only inlcude sites with a large variance. We decided on a standard deviation threshold of 0.1.

```{r prefiltering based on SD}
train.sd <- apply(as.matrix(train.data), MARGIN = 2,FUN = sd)
sd(train.sd)
hist(train.sd)
abline(v = mean(train.sd))

# filter CpG sites with low s.d: only keep those with s.d higher than the average s.d across all CpG sites
train.gsd <- subset(train.sd, train.sd > 0.10)
str(train.gsd)
hist(train.gsd)

train.data.gsd <- train.data[,colnames(train.data) %in% names(train.gsd)]
```

We reduced the # of features to 'r ncol(train.data.gsd)' to reduce computation time. train.data.gsd is the working dataset

# Step 1: Unsupervised clustering:
---------------this section is exploratory analysis, we should move this to Nivi's exploratory file----------
As Rob suggested, PCA should be the precursor to supervised classification, more like an exploration.

## PCA on training data:

```{r pca}
pc.train <- prcomp(scale(train.data,center=T,scale = T))

# look at the eigenvalues
plot(pc.train, type = "l")
```
The `plot()` function returns a plot of the variances (y-axis) associated with the PCs (x-axis), which is useful to decide how many PCs to retain for further analysis.

```{r pca.summary}
summary(pc.train)
```
The `summary()` function describes the importance of the PCs. The first row describe again the standard deviation associated with each PC. The second row shows the proportion of the variance in the data explained by each component while the third row describe the cumulative proportion of explained variance.

```{r plot PCs}
g <- ggbiplot(pc.train, obs.scale = 1, var.scale = 1, 
              groups = train.design$Ethnicity, ellipse = TRUE, 
              circle = TRUE)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', 
               legend.position = 'top')
print(g)



PC123 <- data.frame(pc.train$x[,1:5])              # Take out first 3 PCs
PC123 <- PC123 %>% tibble::rownames_to_column('Samplename') %>%             # Put sample names into column to 
                    left_join(train.design, 'Samplename')                         # Join the metadata info 
head(PC123)            

ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = Ethnicity)) +
  ggtitle('PC1 and PC2: Ethnicity')

ggplot(PC123, aes(x = PC1, y = PC3)) + 
  geom_point(aes(color = Ethnicity)) +
  ggtitle('PC1 and PC3: Ethnicity')

ggplot(PC123, aes(x = PC2, y = PC3)) + 
  geom_point(aes(color = Ethnicity)) +
  ggtitle('PC2 and PC3: Ethnicity')
```

We can see from plotting the first three principal components that our groups (Asian, Caucasian) do not seem to separate. This indicates that the main drivers of the variance in the data is something else.

```{r Plot other metadata}
ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = sex)) +
  ggtitle('Sex')

ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = ga)) +
  ggtitle('Gestational Age')

ggplot(PC123, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = Sample_Group)) +
  ggtitle('Sample Group')
```

It's not clear that our other variables are driving the variance in the data (sex, gestational age, and sample group).

```{r Scatter plot matrix first 5 PCs}
# scatter plot matrix for the first 5 PCs
splom(PC123[,c(2:6,10)], raster = TRUE)
```

Plotting scatter plots of the top 5 PCs against ethnicity, none of the PCs can clearly separate samples by ethnicity, disappointing.

## PCA projection of loadings to test data:

We can use the predict function if we observe new data and want to predict their PCs values.
(Since PCs cannot distinguish ethnicities this step is not useful)
```{r}
# predict PCs for test set by projecting PC loadings to test data
predict(pc.train, new.data = test.data)
```

--------End of exploratory Analysis-----------

# Step 2: Supervised classification:


## logistic regression with elastic net regularization

```{r rename}
#renamed just so that I can copy Amrit's code

x.train <- train.data.gsd
#x.train <- train.data
y.train <- train.design$Ethnicity 
```

```{r subset data for faster run time, eval = FALSE, include = FALSE}
# since the data is very large (~450k rows), I will subset the data first to be able to play around with the code quickly.

x.train <- train.data.gsd[,1:1000] #takes the first 1000 columns (cpgs)
```

### try to identify sites significantly associated with ethnicity based on single marker testing
We can use a linear model to identify differentially methylated probes with `limma`, as shown in this older 540 [seminar](http://www.ugrad.stat.ubc.ca/~stat540/seminars/seminar08_methylation.html).

Create a design matrix.

```{r design matrix}
des.mat <- model.matrix(~Ethnicity, train.design)
```

Fit model to obtain top differentially methylated CpG sites.

```{r fit linear model}

DMRfit <- lmFit(as.data.frame(t(x.train)), des.mat)
DMRfitEb <- eBayes(DMRfit)
cutoff <- 0.1
DMR <- topTable(DMRfitEb, coef = "EthnicityCaucasian", number = Inf, p.value = cutoff)
library(knitr)

knitr::kable(head(DMR))  # top hits 
nrow(DMR)

# subset only those with FDR < 0.1
x.train <- train.data[,colnames(train.data)%in% rownames(DMR)]
```

So using a cutoff of FDR = 0.01, we identified 106 CpG sites that are differentially methylated between Caucasian and Asian genetic ancestry. Now we can make some plots to check these hits.


### glmnet

```{r}
glmfit = glmnet(as.matrix(x.train), y.train, family = "binomial")
plot(cv.glmnet(as.matrix(x.train), y.train,family = "binomial"))
predict(glmfit, test.data[,colnames(test.data)%in% colnames(x.train)], type = "class")

```


### caret
```{r Specify resampling method}
k = 5
M = 3

fitControl <- trainControl(method = "repeatedcv", 
													 number = k,                 # Number of folds
													 repeats = M,
													 ## Estimate class probabilities
													  classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary,
													 savePredictions = TRUE,      # Saves ROC results
													 allowParallel = TRUE
													 )  

netGrid <- expand.grid(alpha = c(0.55, 0.75, 0.9),
                           lambda = c(0.077, 0.25))
# netGrid <- expand.grid(alpha = (1:9)/10,
#                           lambda = seq(0.05,0.5,length.out = 9))
netGrid <- expand.grid(alpha = c(0.75),
                           lambda = c(0.077, 0.25))
# netGrid <- expand.grid(alpha = c(0.1),
#                            lambda = c(0.5))
```


```{r tune glmnet parameters}
set.seed(2017)                                         # training models requires the use of random #s. Setting (set.seed()) the randomness ensures reproducibility


system.time(netFit <- train(x = x.train,   # samples need to be in rows, features need to be columns
								y = y.train,                  
								method = "glmnet",                     # glmnet model
								trControl = fitControl,                # use fitControl to specify cross validation
								tuneGrid = netGrid,
								preProcess = c( "center", "scale"),    # Center and Scale the data
								metric = 'ROC')                        # ROC because distribution is slightly skewed
)

netFit
length(predictors(netFit))

probTest <- predict(netFit, x.train, type = 'prob')
ethProb <- probTest[,'Asian']
ethProb
#saveRDS(netFit, './Data/Processed Data/netFit_alpha55_lambda_077_51pred.rds')
```

Cross validation with a fold of k = 5 (making each fold 9 samples large), was used to determine the optimal tuning parameters.

Horvath et al. (2013) uses an 'elastic net generalized linear model' to build an across-tissue DNAm predictor on age. Since our data is the same type, we'll try glmnet.

Horvath, S. (2013). DNA methylation age of human tissues and cell types. Genome Biology, 14(10), R115. http://doi.org/10.1186/gb-2013-14-10-r115
 
```{r examine results}
trellis.par.set(caretTheme())
ggplot(netFit)
ggplot(svmFit)

#heatmap of results
plot(netFit, metric = "ROC", plotType = "level",
     scales = list(x = list(rot = 90)))

plot(svmFit, metric = "ROC", plotType = "level",
     scales = list(x = list(rot = 90)))
```

```{r extract features}
predictorsNet <- predictors(netFit)
length(predictorsNet)

predictorsSvm <- predictors(svmFit)
length(predictorsSvm)
```
Looks like our model has chosen 'r length(predictors)' CpGs that can be used to predict ethnicity.

```{r plot top 35}
glmImp <- varImp(netFit, scale = F) # gives the t-statistic for all CpGs in the dataset
plot(glmImp, top = 51)
```

```{r Estimate test error by nested cv}
# this is adapted from Amrit's code from lec 19
#use repeated CV to estimate test performance
set.seed(2018)

# list of lists containing fold ids
folds <- lapply(1:M, function(i) createFolds(y.train, k = k))

system.time(netTesterror <- lapply(folds, function(i){
  lapply(i, function(j){
    # tune parameters with CV
    set.seed(2019)
    fitControl <- trainControl(method = "repeatedcv", 
													 number = k,                 
													 repeats = M,
													 classProbs = TRUE,
                           summaryFunction = twoClassSummary,
													 savePredictions = TRUE,
													 allowParallel = T)
    
    
    # build elastic net classifier
    netFit <- train(x =  x.train[-j,],   
								y = y.train[-j],                  
								method = "glmnet",                     
								trControl = fitControl,
								preProcess = c( 'center', 'scale'),
								metric = 'ROC')   
    
    # Estimate probabilities of test predictions
    probTest <- predict(netFit, x.train[j,], type = 'prob')
    ethProb <- probTest[,'Asian']
    ethProb
  })

})
)
# netTesterror
saveRDS(netTesterror,"netTesterror.rds")
```

```{r Performance}
# Computer classification performance measures
# enet
Performance <- mapply(function(x, y){
  auc <- pROC::roc(y.train[unlist(x)], unlist(y),
                   direction ='<',
                   levels = c('Caucasian', 'Asian'),
                   percent = TRUE)
  list(tpr = auc$sensitivities,
       fpr = 100 - auc$specificities,
       auc = round(auc$auc, 2))
}, x = folds, y = netTesterror)
Performance
```

```{r ROC curve}
# plot ROC curve

plot(Performance['tpr',][[1]] ~ Performance['fpr',][[1]],
     type = 'l', col = 1, xlab = '100 - sensitivity',
     ylab = 'Sensitivity', main = 'Enet')
for(i in length(folds)){
  points(Performance['tpr',][[i]] ~ Performance['fpr',][[i]],
         type = 'l', col = 2)
}
text(x = 60, y = 40, labels =
       paste0('mean AUC = ', round(mean(unlist(Performance['auc',])), 1),
              '+/-', round(sd(unlist(Performance['auc',])), 1), '%'))
```

## Support Vector Machine

```{r train svm}
# Linear kernel SVM with CV
    fitControl <- trainControl(method = "cv", 
													 number = k,                 
													 classProbs = TRUE,
                           summaryFunction = twoClassSummary,
													 savePredictions = TRUE,
													 allowParallel = T)
    
    
    # build SVM classifier
    svm.fit <- train(x=x.train,
                        y= y.train,
                        method = "svmLinear",
                        preProc = c("center","scale"),
                        metric="ROC",
                        trControl=fitControl)

    svm.fit
```


```{r Estimate test error by nested cv}
# this is adapted from Amrit's code from lec 19
#use repeated CV to estimate test performance
set.seed(2018)

# list of lists containing fold ids
folds <- createFolds(y.train, k = k)

system.time(svmTesterror <- lapply(folds, function(j){
    # tune parameters with CV
    fitControl <- trainControl(method = "cv", 
													 number = k,                 
													 classProbs = TRUE,
                           summaryFunction = twoClassSummary,
													 savePredictions = TRUE, allowParallel = T)
    
    
    # build SVM classifier
    svmFit <- train(x=x.train[-j,],
                        y= y.train[-j],
                        method = "svmLinear",
                        preProc = c("center","scale"),
                        metric="ROC",
                        trControl=fitControl)
    
    # Estimate probabilities of test predictions
    probTest <- predict(svmFit, x.train[j,], type = 'prob')
    ethProb <- probTest[,'Asian']
    ethProb

})
)
#svmTesterror
```

```{r Performance}
# Computer classification performance measures
# enet
Performance <- mapply(function(x, y){
  auc <- pROC::roc(y.train[unlist(x)], unlist(y),
                   direction ='<',
                   levels = c('Caucasian', 'Asian'),
                   percent = TRUE)
  list(tpr = auc$sensitivities,
       fpr = 100 - auc$specificities,
       auc = round(auc$auc, 2))
}, x = folds, y = svmTesterror)
Performance
```

```{r ROC curve}
# plot ROC curve

plot(Performance['tpr',][[1]] ~ Performance['fpr',][[1]],
     type = 'l', col = 1, xlab = '100 - sensitivity',
     ylab = 'Sensitivity', main = 'Linear Kernel SVM')
for(i in length(folds)){
  points(Performance['tpr',][[i]] ~ Performance['fpr',][[i]],
         type = 'l', col = 2)
}
text(x = 60, y = 40, labels =
       paste0('mean AUC = ', round(mean(unlist(Performance['auc',])), 1),
              '+/-', round(sd(unlist(Performance['auc',])), 1), '%'))
```



# Step 3: Predict Ethnicity for Test Set

Use hierarchical clustering to visualize the cluster 
```{r predict ethnicity for test set}
x.test <- test.rmna

predictors(netFit) %in% rownames(test.data)

x.test <- x.test[, colnames(x.test) %in% colnames(train.data.gsd)]
x.test <- x.test[, colnames(x.test) %in% rownames(DMR)]
sum(is.na(x.test)) # there shouldn't be NAs present -- predict() function cannot predict if there are missing values

y.predict <- predict(netFit,  x.test, type = "prob")
y.predict[,"Asian"]
netFit
svm.predict <- predict(svm.fit, x.test, type = "prob")

# Hierarchical clustering of predicted data, distance measured by Euclidean distance, average linkage
distance <- dist(x.test, method="euclidean") 
cluster <- hclust(distance, method="average")
plot(cluster, hang=-1, label=y.predict[,"Asian"])
```

# Step 4: Differentially Methylated Sites Between Test and Control

Our classifier predicts that all test samples are Caucasians. This raises suspicion as we expected some ethnic diversity in our test set. Potential reasons for getting this result include:

* these test samples just all happen to be Caucasians;

* too many predictors for elastic net to select, some features are significantly different by ethnicity in training data by chance but were still selected as predictors, which introduces overfitting;

* the training set and test set are significantly differentially methylated.

We can look at the differentially methylated CpG sites between training and test set to see how much difference these two data sets have.


Which set of CpG sites should we conduct DMA on?

1) all CpGs present in both data sets ( after removing sites with missing values in test);

```{r merge all}
merged.all <- rbind(train.data, test.data[, colnames(test.data) %in% colnames(train.data)])
merged.design <- rbind(train.design[,c("Samplename","ga","sex")], test.design[,c("Samplename","ga","sex")])
merged.design$Group = relevel(
    factor(c(rep("Train",nrow(train.data)),rep("Test", nrow(test.data)))), 
    ref = "Train")
DesMat <- model.matrix(~ Group + ga + sex, merged.design)

```


2) only CpGs with s.d. > 0.1;


```{r merged filtered}
x.test <- test.rmna
x.test <- x.test[, colnames(x.test) %in% colnames(train.data.gsd)]

merged.sd <- rbind(x.train, x.test)

```


3) only CpGs identified as predictors using elastic net;


```{r merge predictors}
merged.pred <- merged.sd[, colnames(merged.sd) %in% predictors(netFit)]
```

## Unsupervised Clustering PCA

```{r merged PCA}
pc.merged <- prcomp(merged.all, center=T, scale = T)
PC1to5 <- data.frame(pc.merged$x[,1:5])              # Take out first 5 PCs
PC1to5 <- PC1to5 %>% tibble::rownames_to_column('Samplename') %>%             # Put sample names into column to 
                    left_join(merged.design, 'Samplename')                         # Join the metadata info 
head(PC1to5)            

# scatter plot matrix for the first 5 PCs
png("merged_pca.png", width = 1000, height = 1000)
splom(PC1to5[,c(2:6,9)], raster = TRUE)
# first PC is significantly different!
dev.off()
# scree plot
png("merged_pca_scree.png", width = 600, height = 600)
plot(pc.merged, type = "l", main = "PCA Scree Plot for Merged Data")
dev.off()
```

The first PC differentiates training and test set, which means there are systematic differences between the two sets. We must discard the top PC first before predicting test set.

```{r discard the first pc}

Xhat<- pc.merged$x[,-1] %*% t(pc.merged$rotation[,-1])  # discard the first PC

merged.trunc <- scale(Xhat, center = F, scale = 1/pc.merged$scale)
merged.trunc <- scale(merged.trunc, center = -1 * pc.merged$center, scale = FALSE) # back-scale features to original center and scale

str(merged.trunc)

# verify PC truncation
pc.trunc <- prcomp(merged.trunc, center=T, scale = T)

PC1to5.trunc <- data.frame(pc.trunc$x[,1:5])              # Take out first 5 PCs
PC1to5.trunc <- PC1to5.trunc %>% tibble::rownames_to_column('Samplename') %>%             # Put sample names into column to 
                    left_join(merged.design, 'Samplename')                         # Join the metadata info 

# scatter plot matrix for the first 5 PCs
splom(PC1to5.trunc[,c(2:6,9)], raster = TRUE)

```


## Differential Methylation Analysis with Single Markers

### All Sites
```{r merged.all DMA}
DMRfit <- lmFit(as.data.frame(t(merged.all)), DesMat)
DMRfitEb <- eBayes(DMRfit)
cutoff <- 0.05
DMR <- topTable(DMRfitEb, coef = 'GroupTest', number = Inf, p.value = cutoff)
head(DMR)
nrow(DMR)
```

A total of `nrow(DMR)`sites are differentially methylated between training and test set with FDR < 0.05.

## Re-do Elastic Net Logistic Regression

```{r redo filtering sd}

x.train.redo <- as.data.frame(merged.trunc[1:45,])

train.sd.redo <- apply(as.matrix(x.train.redo), MARGIN = 2,FUN = sd)

# filter CpG sites with low s.d: only keep those with s.d higher than the average s.d across all CpG sites
train.gsd.redo <- subset(train.sd.redo, train.sd.redo > 0.10)
str(train.gsd.redo)

train.data.gsd.redo <- x.train.redo[,colnames(x.train.redo) %in% names(train.gsd.redo)]

x.train.redo <- train.data.gsd.redo
```

```{r}

netGrid <- expand.grid(alpha = c(0.55, 0.75, 0.9),
                           lambda = c(0.077, 0.25))
netGrid <- expand.grid(alpha = (1:9)/10,
                          lambda = seq(0.05,0.5,length.out = 9))
netGrid <- expand.grid(alpha = c(0.75),
                           lambda = c(0.077, 0.25))
# netGrid <- expand.grid(alpha = c(0.1),
#                            lambda = c(0.5))
```


```{r tune glmnet parameters}
set.seed(2017)                                         # training models requires the use of random #s. Setting (set.seed()) the randomness ensures reproducibility


system.time(netFit.redo <- train(x = x.train.redo,   # samples need to be in rows, features need to be columns
								y = y.train,                  
								method = "glmnet",                     # glmnet model
								trControl = fitControl,                # use fitControl to specify cross validation
								tuneGrid = netGrid,
								preProcess = c( "center", "scale"),    # Center and Scale the data
								metric = 'ROC')                        # ROC because distribution is slightly skewed
)

netFit.redo
length(predictors(netFit.redo))

probTest.redo <- predict(netFit.redo, x.train.redo, type = 'prob')
ethProb.redo <- probTest[,'Asian']
ethProb.redo

```

```{r predict redo}
x.test.redo <- merged.trunc[46:97,]

x.test.redo <- x.test.redo[, colnames(x.test.redo) %in% colnames(x.train.redo)]

y.predict.redo <- predict(netFit.redo,  x.test.redo, type = "prob")
y.predict.redo[,"Asian"]
y.predict.redo
```


# step 6: weighted loss function; up-sampling; M value instead of beta value; correlation between train and test for identified predictors