---
title: "ModelBuilding"
author: "Victor, Ming"
date: "April 2, 2017"
output: 
  github_document:
    toc: TRUE
    toc_depth: 2
---
# S0 Set up workspace
## Load packages
```{r load packages, message=FALSE}
#source("https://bioconductor.org/biocLite.R")
#biocLite('e1071')                                    # required for glmnet in caret
#biocLite('pROC')
library(pROC)
library(ggplot2)
library(limma)
library(caret)
library(dplyr)
library(parallel)
library(doParallel)
```
## Parallel processing
Set up parallel processing to speed up trian()
Make sure to specify in trainControl()

```{r parallel processing, message=FALSE}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```
## Load data
Read in pre-processed data:
*Make sure the pre-processed data (data.txt, which is in data.zip) is present in the ../processed_data/ directory.

```{r load data}
setwd('../')                                           # note: all of these relative file path calls work only for knitting

# load data (pre-processed training set)
train.data <- read.table('../data/Processed Data/data.txt')
str(train.data)                                   ## row names are CpG sites, column names are sample names
# transpose our data to have rows as observations, which is more convenient later on for building models
train.data <- as.data.frame(t(train.data))

# load metadata
train.design <- read.csv("../data/Processed Data/des.txt", sep="\t", header=TRUE)
str(train.design)

row.names(train.data) == train.design$Samplename               # check that the samples are in same order
```

Read in test data:
```{r read test}
setwd('../') # for knitting
# read pre-processed test data
test.data <- read.table("../Data/Processed Data/Test data/Matrix.processed.betas.placenta.txt", row.names = 1, header = T)
test.data <- as.data.frame(t(test.data))   #transpose data

# meta data for test data
test.design <-  read_excel("../../data/Processed Data/Test data/metadata.GA_illumina_methylation.xls", 
    sheet = "Metadata", skip = 28)

# subset only columns we need and rename them
test.design <- test.design[test.design$`Sample name` %in% rownames(test.data),]
test.design <- test.design[,c(1,7,8,10)]
colnames(test.design)[1] <- "Samplename"
colnames(test.design)[3] <- "sex"
colnames(test.design)[4] <- "ga"

str(test.design)

```

# S1 Prefiltering Features

Reducing the number of features can significantly reduce computational time, which is desirable when the dataset is large. However, we must be careful not remove potentially 'interesting' features that have a high chance of being useful in building a classifier.

## 1.1 Remove NAs

We should remove any sites with NAs in them or else predictions cannot be generated for these samples if the CpG site is chosen as a predictor.
```{r remove NAs}
#remove sites with NAs
sum(is.na(test.data)) # 52000 total entries that are NA
test.rmna <- test.data[, colSums(is.na(test.data)) == 0]  # remove columns with NAs present
```
## 1.2 Reduce CpGs to match test and train
Some CpGs were removed in the Test dataset from preprocessing and QC. These need to be removed, or errors might occur when trying to predict.

```{r filter training data to contain the same CpGs as the test}
# this isn't necessary if the test data didn't have CpGs removed (as a result of QC/preprocessing)
train.data <- train.data[,colnames(train.data) %in% colnames(test.rmna)]
```

## 1.3 Prefiltering cpgs (most variable) 
The goal of this prefiltering section is to reduce computational time without compromising detecting interesting features.

```{r prefiltering based on SD}
train.sd <- apply(as.matrix(train.data), MARGIN = 2,FUN = sd) #caculate SD for each feature
hist(train.sd)                    # histogram of the s.d.'s
abline(v = mean(train.sd)) 

# filter CpG sites with low s.d: only keep those with s.d higher than the average s.d across all CpG sites
train.gsd <- subset(train.sd, train.sd > 0.10)
hist(train.gsd)

# subset training data to only highly variable features
train.data.gsd <- train.data[,colnames(train.data) %in% names(train.gsd)]
```

We reduced the # of features to `r ncol(train.data.gsd)` to reduce computation time. `train.data.gsd` is the working dataset.
# S2 Supervised classification:
We decided to try two different models for building our classifer: elastic net logistic regression (`glmnet`) and support vector machines (SVM). Both of these models have been used in the literature to build predictive models based on 450k DNA methylation data (Horvath 2013, De Carli et al 2017), indicating that they may be well-suited for our dataset.
## 2.1 logistic regression with elastic net regularization
```{r rename}
#renamed training data for standard coding
x.train <- train.data.gsd
y.train <- train.design$Ethnicity 
```

```{r subset data for faster run time, eval = FALSE, include = FALSE}
# This subsetting is for testing code out without computational delay
# since the data is very large (~450k rows), I will subset the data first to be able to play around with the code quickly.
x.train <- train.data.gsd[,1:1000] #takes the first 1000 columns (cpgs)
```

```{r Specify resampling method}
k = 5
M = 3

fitControl <- trainControl(method = "repeatedcv", 
													 number = k,                 # Number of folds
													 repeats = M,
													 ## Estimate class probabilities
													 classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary,
													 allowParallel = TRUE # allow parallel processing
													 )  

netGrid <- expand.grid(alpha = c(0.75),
                           lambda = c(0.077, 0.25))
```
We specify the model to be built using repeated cross validation with a fold = 5, and repeats = 3. We tune the model holding alpha constant (alpha = 0.75), keeping alpha high to favour L1 norm to achieve a small panel of biomarkers. Lambda, the magnitude of the penalty, is tested at 0.077, and 0.25.

```{r tune glmnet parameters}
set.seed(2017)                                         # training models requires the use of random #s. Setting (set.seed()) the randomness ensures reproducibility

system.time(netFit <- train(x = x.train,   # samples need to be in rows, features need to be columns
								y = y.train,                  
								method = "glmnet",                     # glmnet model
								trControl = fitControl,                # use fitControl to specify cross validation
								tuneGrid = netGrid,
								preProcess = c( "center", "scale"),    # Center and Scale the data
								metric = 'Accuracy')                        # ROC because distribution is slightly skewed
)

netFit
netFit$results
#saveRDS(netFit, './Data/Processed Data/netFitfinal.rds')
```

```{r extract features}
predictorsNet <- predictors(netFit)
length(predictorsNet)
#write.table(predictorsNet, './Data/Processed Data/predictorsGlmnet.txt')
```
Our glmnet-built model has chosen `r length(predictorsNet)` CpGs that can be used to predict ethnicity.

## 2.2 SVM with linear kernel
This section is for building the model using SVM with a linear kernel (i.e. penalty parameter C = 1). However, because computational time is long, this section is excluded when ran, since we have chosen the glmnet model to be our final model.

```{r svm linear, eval = TRUE}
svmControl <- trainControl(method="repeatedcv",   
                           number = 5,
                           repeats=3,		    
                           summaryFunction=twoClassSummary,	# Use AUC to pick the best model
                           classProbs=TRUE,
                           allowParallel = TRUE)

system.time(svmFit <- train(x=x.train,
                            y= y.train,
                            method = "svmLinear",
                            preProc = c("center","scale"),
                            metric="ROC",
                            trControl= svmControl)	)
svmFit
```
# S3 Predict Ethnicity for external data Set
Next, we use the models we built and run it on an external data set, where there is no ethnicity information.

## 3.1 glmnet
Using the `predict()` function we can obtain both binary class prediction results as well as probabilities of being "Asian" under logistic regression model.

```{r predict ethnicity for test set}
#subset x.test down to the sites used for training (after prefilter)
x.test <- test.data[,colnames(test.data) %in% names(x.train)]

#class predictions
y.predictNet <- predict(netFit,  x.test)
y.predictNet

#class probabilities
y.predictNetProb <- predict(netFit, x.test, type = 'prob')
y.predictNetProb

#saveRDS(y.predictNet, './data/Processed Data/y_predictNet.rds')
```
It looks like our model classifies the entire external dataset is Caucasian. This is suspicious, as we believe the samples to come from a relatively heterogenous population. However, due to time constraints, we decided to move ahead and perform downstream analysis. If there was more time, we might think about where we can change our modelling process to produce more sensible results. 

#### Some explanations for this result:
- It's possible that the data set is truly all Caucasian.

- There are just too many predictors for elastic net to select, some features can be significantly different by ethnicity in training data by chance but were still selected as predictors, which introduces overfitting;

- The test dataset is too 'different' to have the classifier ran on. (mostly differentially methylated between test and training set across all sites)

- The self-reported ethnicities in the training data is unreliable.

## 3.2 SVM 
```{r SVM predict on test}
y.predictSVM <- predict(svmFit,  x.test)

y.predictSVM
```

# S4 Analysis of Predictors
Here we pull out the CpG sites and look at them more closely. First we will see if clustering with only the predictors separates asians and caucasians
## 4.1 Clustering
```{r load package, message = FALSE}
library(ggdendro)
library(sparcl) # ColorDendrogram
library(dendextend)
```

```{r clustering train based on predictors, warning = FALSE}
#without all CpGs used to train
hclust <- hclust(dist(x.train, method = 'euclidean'))

#swap labels with ethnicity
swaplabels <- function(hclust, des){     # des is a design matrix containing 'Samplename' and 'Ethnicity' col
  labels <- data.frame(labels(hclust))   # pulls out current labels (samplename)
  colnames(labels) <- 'Samplename'
  labels <- labels %>% left_join(select(des, Samplename, Ethnicity), by = 'Samplename')
  labels(hclust) <- as.character(labels$Ethnicity)
  return(hclust)
}

hclust <- swaplabels(hclust, train.design)
y1 = cutree(hclust, 3)
ColorDendrogram(hclust, y = y1, labels = names(y1), branchlength = 1.0, main = 'Clustering train on all CpGs')

#with predictors only
x.train.predictors <- x.train[,colnames(x.train) %in% predictorsNet]
hclust2 <- hclust(dist(x.train.predictors, method = 'euclidean'))
hclust2 <- swaplabels(hclust2, train.design)          #swap labels with ethnicity
y2 = cutree(hclust2, 2)
ColorDendrogram(hclust2, y = y2, labels = names(y2), branchlength = 0.3, main = 'Clustering train with predictors only')
```
We see that clustering with the predictors extracted from our classifier, our training data clusters into two homogenous groups consisting of Asians and Caucasians. This might indicate overfitting, as there were 0 missclassifications.
```{r Clustering test data, warning=FALSE}
# Hierarchical clustering of predicted data, distance measured by Euclidean distance, average linkage
hclust3 <- hclust(dist(x.test, method = 'euclidean'))
y3 = cutree(hclust3, 2)
ColorDendrogram(hclust3, y=y3, labels = names(y3), branchlength = 2, main = 'Clustering Test with all CpGs')

# clustering only with the predictors
x.test.predictors <- x.test[,colnames(x.test) %in% predictorsNet]
hclust4 <- hclust(dist(x.test.predictors, method = 'euclidean'))
y4 = cutree(hclust4, 2)
ColorDendrogram(hclust4, y=y4, labels = names(y4), branchlength = 0.25, main = 'Clustering Test with predictors only')
```
We can see that clustering the external data set (test) does not improve the separation of the test data into two main clusters, indicating that the classifier is not producing a heterogenous set of predictions.

```{r cluster both test and train, warning = FALSE}
# add samplename column to match on
x.test.predictors <- x.test.predictors %>% 
                        tibble::rownames_to_column('Samplename')
x.train.predictors <- x.train.predictors %>%
                        tibble::rownames_to_column('Samplename') 

# replace sample name with true ethnicity info
#x.train.predictors <- x.train.predictors %>% left_join(select(train.design, Samplename, Ethnicity), by = 'Samplename') 
#x.train.predictors$Samplename <- x.train.predictors$Ethnicity 

# combine train and test
x.test.train.predictors <- full_join(x.train.predictors, x.test.predictors) %>%
                            tibble::column_to_rownames('Samplename')
# clustering
hclust5 <- hclust(dist(x.test.train.predictors, method = 'euclidean'))
labels5 <- data.frame(labels(hclust5))   # pulls out current labels (samplename)
colnames(labels5) <- 'Samplename'
labels5 <- labels5 %>% left_join(select(train.design, Samplename, Ethnicity), by = 'Samplename')
#replace train samples with ethnicity labels
#labels5$Samplename[!is.na(labels5$Ethnicity)] <- as.character(labels5$Ethnicity[!is.na(labels5$Ethnicity)])

labels(hclust5) <- labels5$Samplename


hclust5 <- swaplabels(hclust5, train.design)
labels(hclust5)

y5 = cutree(hclust5, 5)
ColorDendrogram(hclust5, y = y5, labels = names(y5), branchlength = 0.3, main = 'Clustering train with predictors only')

```
When we perform hierarchical clustering with the entire train and test set, we can see that Caucasians and Asians mainly separate into the two largest clusters, with the majority of the test set (unlabeled branches) clustering closer to the Caucasians samples. 

## 4.2 Plot CpG Predictors
```{r plot top 35}
glmImp <- varImp(netFit, scale = F) # gives the t-statistic for all CpGs in the dataset
plot(glmImp, top = length(predictors(netFit)))
```
Here we plot the 11 predictor CpGs against 'importance' which is calculated based on their relative t-statistic score.
```{r plotting CpGs, warning = FALSE}
# For training data set
cpg1 <- x.train.predictors %>% select(Samplename, cg16329197) %>% 
                                left_join(train.design, 'Samplename')
ggplot(cpg1, aes(x=Ethnicity, y=cg16329197))+
  geom_boxplot()+
  ggtitle('Top CpG predictor methylation in Training data is differentially
          methylated')+
  ylab('cg16329197 methylation')

# Pick 11th ranked CpG
cpg2 <- x.train.predictors %>% select(Samplename, cg22853943) %>% 
                                left_join(train.design, 'Samplename')
ggplot(cpg2, aes(x=Ethnicity, y=cg22853943))+
  geom_boxplot()+
  ggtitle('11th ranked CpG predictor methylation in Training data is
          differentially methylated')+
  ylab('cg22853943 methylation')
```
We can see that the 1st and 11th ranked predictor CpG are both obviously differentially methylated in the training dataset between the asians and caucasians. This is a good sign that the model has chosen 'useful' CpG sites. However, perhaps these CpGs fit our training data too well.

# S5 Tune alpha and lambda 10 x 10 grid

This section is to tune parameters across 100 different combinations of alpha and lambda

```{r tune glmnet parameters 10 by 10, eval = TRUE, include = TRUE}
netGrid100 <-  expand.grid(alpha = c(0.10, 0.20, 0.30, 0.40, 0.50, 
                                     0.60, 0.70, 0.80, 0.90, 1.00),
                           lambda = c(0.025, 0.050, 0.075, 0.10, 0.15, 
                                      0.20, 0.25, 0.30, 0.40, 0.50))

set.seed(2017)                              

system.time(netFit100 <- train(x = x.train, 
								y = y.train,                  
								method = "glmnet",                     # glmnet model
								trControl = fitControl,                # use fitControl to specify cross validation
								tuneGrid = netGrid100,
								preProcess = c( "center", "scale"),    # Center and Scale the data
								metric = 'ROC')                        # ROC because distribution is slightly skewed
)
netFit100
```

```{r examine CV, eval = TRUE}
trellis.par.set(caretTheme())
ggplot(netFit100)
#heatmap of results
plot(netFit100, metric = "ROC", plotType = "level",
     scales = list(x = list(rot = 90)))
glmImp100 <- varImp(netFit100, scale = F) # gives the t-statistic for all CpGs in the dataset
plot(glmImp100, top = length(predictors(netFit100)))
length(predictors(netFit100))
```

# S6 Re-do Modeling and Prediction by Homogenizing Training and Test Sets

Following the discussions in step 3, we aim to re-do our model fitting and prediction by first assessing how different is the test set from our training data using PCA. Then we attempt to homogenize the two datasets by removing the top PC. After homogenization, we re-fit elastic net logistic regression with the transformed training set and re-predict the transformed test set.

## 6.1 Merge Test Set with Training Set

Merge the two datasets, only CpGs present in both sets are kept.

```{r merge all}
merged.all <- rbind(train.data, test.data[, colnames(test.data) %in% colnames(train.data)])
merged.design <- rbind(train.design[,c("Samplename","ga","sex")], test.design[,c("Samplename","ga","sex")])
merged.design$Group = relevel(
    factor(c(rep("Train",nrow(train.data)),rep("Test", nrow(test.data)))), 
    ref = "Train")

```

## 6.2 Unsupervised Clustering PCA

First we perform PCA on the merged dataset to spot potential systematic differences between test vs. training set. PCA is performed for the correlation matrix.

```{r merged PCA}
pc.merged <- prcomp(merged.all, center=T, scale = T) # perform PCA on correlation matrix
PC1to5 <- data.frame(pc.merged$x[,1:5])              # Take out first 5 PCs
PC1to5 <- PC1to5 %>% tibble::rownames_to_column('Samplename') %>%             # Put sample names into a column 
                    left_join(merged.design, 'Samplename')                         # Join the metadata info 

summary(pc.merged)
# first PC can explain 31.5% variance

# scree plot
plot(pc.merged, type = "l", main = "PCA Scree Plot for Merged Data")

# scatter plot matrix for the first 5 PCs
splom(PC1to5[,c(2:6,9)], raster = TRUE)
# first PC is significantly different between training vs. test

```

The first PC differentiates training and test set, which means there are systematic differences between the two sets. We must discard the top PC first before predicting test set. This is done by:

1. Reconstruct the (centered and scaled) merged dataset by discarding the top PC;

2. Re-scale and re-center the correlation matrix for the reconstructed merged data;

```{r discard the first pc}
# discard the first PC
Xhat<- pc.merged$x[,-1] %*% t(pc.merged$rotation[,-1]) 

# back-scale features to original center and scale
merged.trunc <- scale(Xhat, center = F, scale = 1/pc.merged$scale)
merged.trunc <- scale(merged.trunc, center = -1 * pc.merged$center, scale = FALSE)
str(merged.trunc)

# verify that PC truncation was sucessful
pc.trunc <- prcomp(merged.trunc, center=T, scale = T)

PC1to5.trunc <- data.frame(pc.trunc$x[,1:5])              # Take out first 5 PCs
PC1to5.trunc <- PC1to5.trunc %>% tibble::rownames_to_column('Samplename') %>%             # Put sample names into a column
                    left_join(merged.design, 'Samplename')                         # Join the metadata info
# scatter plot matrix for the first 5 PCs
splom(PC1to5.trunc[,c(2:6,9)], raster = TRUE)
# no more separation between test and training
```

## 6.3 Re-do Elastic Net Logistic Regression

After discarding the top PC for merged data, we separate training and test dataset again and use the homogenized training set to re-fit the logistic regression model with elastic net.

```{r redo filtering sd}
# subset only training samples
x.train.redo <- as.data.frame(merged.trunc[1:45,])

# filter low variances features
train.sd.redo <- apply(as.matrix(x.train.redo), MARGIN = 2,FUN = sd)
train.gsd.redo <- subset(train.sd.redo, train.sd.redo > 0.10)
train.data.gsd.redo <- x.train.redo[,colnames(x.train.redo) %in% names(train.gsd.redo)]

x.train.redo <- train.data.gsd.redo
```

```{r tuning grid}
netGrid <- expand.grid(alpha = (1:9)/10,
                          lambda = seq(0.05,0.5,length.out = 9))
netGrid <- expand.grid(alpha = c(0.75),
                           lambda = c(0.077, 0.25))
```

Re-fit the `glmnet` model:

```{r tune glmnet parameters}
set.seed(2017)                                         # training models requires the use of random #s. Setting (set.seed()) the randomness ensures reproducibility


system.time(netFit.redo <- train(x = x.train.redo,   # samples need to be in rows, features need to be columns
								y = y.train,                  
								method = "glmnet",                     # glmnet model
								trControl = fitControl,                # use fitControl to specify cross validation
								tuneGrid = netGrid,
								preProcess = c( "center", "scale"),    # Center and Scale the data
								metric = 'ROC')                        # ROC because distribution is slightly skewed
)

netFit.redo
length(predictors(netFit.redo))

# Histogram of probability(Asian)
probTest.redo <- predict(netFit.redo, x.train.redo, type = 'prob')
ethProb.redo <- probTest.redo[,'Asian']
hist(ethProb.redo)

```

Predict the homogenized test set:

```{r predict redo}
# subset the test set
x.test.redo <- merged.trunc[46:97,]

x.test.redo <- x.test.redo[, colnames(x.test.redo) %in% colnames(x.train.redo)]

# classification

y.predict.redo <- predict(netFit.redo,  x.test.redo)
y.predict.redo[,"Asian"]

# predicted probability to be Asian
y.predict.redo <- predict(netFit.redo,  x.test.redo, type = "prob")
y.predict.redo[,"Asian"]
# histogramfor the prob. to be Asian
hist(y.predict.redo, main = "Predicted Probability to be Asian")
```

