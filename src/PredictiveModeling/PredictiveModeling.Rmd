---
title: "Predictive Modeling Analysis"
author: "Ming Wan, Victor Yuan"
output: 
  github_document:
    toc: TRUE
---

# Step 0: Load Packages and Data
Load required packages:


```{r}
library(ggplot2)
library(limma)
library(caret)
library(dplyr)
library(glmnet)
```

Read in pre-processed data:
*Make sure the pre-processed data (data.txt, which is in data.zip) is present in the ../processed_data/ directory.

```{r}
setwd('../')                                           # note: all of these relative file path calls work only for knitting
# pre-processed training set
train.data <- read.table('../data/processed_data/data.txt')

str(train.data)
## row names are CpG sites, column names are codings of sample

# design matrix
design <- read.csv("../data/processed_data/des.txt", sep="\t", header=TRUE)

str(design)
## sample names column is different from methyl data columns, need to correct 
```

# Step 1: Unsupervised clustering:

As Rob suggested, PCA should be the precursor to supervised classification, more like an exploration.

## PCA on training data:

```{r pca}
pc.train <- prcomp(train.data, center = T, scale = T)

# look at the eigenvalues
plot(pc.train)
pc.train$sdev
## apparently the first PC can explain most of the variances in our data
diag((pc.train$sdev)^2)
sum(diag((pc.train$sdev)^2))
diag((pc.train$sdev)^2)[1,1]/sum(diag((pc.train$sdev)^2))

# first 2 PCS
PC12 <- data.frame(pc.train$rotation[,c("PC1","PC2")])              # Take out first 2 PCs
PC12 <- PC12 %>% tibble::rownames_to_column('Samplename') %>%       # Put sample names into column to match on
                    left_join(design, 'Samplename')                 # Join the metadata info 
head(PC12)

ggplot(PC12, aes(x = PC1, y = PC2)) + 
  geom_point(aes(color = Ethnicity))
## to do: add title

# scatter plot matrix for the first 5 PCs
splom(pc.train$rotation[,1:5], panel = panel.smoothScatter, raster = TRUE)
```

## PCA projection of loadings to test data:

```{r}
# read pre-processed test data

# project PC loadings to test data

```


# Step 2: Supervised classification:


## logistic regression with elastic net regularization

```{r logit}

```

